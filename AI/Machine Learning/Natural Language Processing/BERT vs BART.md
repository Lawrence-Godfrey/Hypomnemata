### BERT
- BERT (Bidirectional Encoder Representations from Transformers) is a [[transformer]]-based model that was introduced by Google in 2018.
- BERT is trained on a task called masked language modelling.
- BERT is known for its excellent performance on tasks that require understanding the context and relationships between words, such as question answering and sentiment analysis. BERT's pre-training task encourages the model to learn representations that are sensitive to the context in which words appear.
- BERT uses a [[transformer]] architecture with a [[multi-layer encoder]].
### BART
- BART (Denoising Autoencoder from Transformer) is a [[transformer]]-based model that was introduced by Facebook AI in 2020.
- BART is trained on a task called denoising autoencoding, where the input text is corrupted with random tokens, and the model is trained to reconstruct the original text.
- BART is known for its excellent performance on tasks that require handling complex language, such as text summarisation and machine translation. BART's pre-training task encourages the model to learn representations that are robust to noise and variations in the input text.
- BART uses a [[transformer]] architecture with a [[multi-layer encoder-decoder]].