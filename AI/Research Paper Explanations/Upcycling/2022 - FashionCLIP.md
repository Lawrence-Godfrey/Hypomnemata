https://arxiv.org/abs/2204.03972

- Introduces FashionCLIP, a CLIP-based model explicitly trained to produce product embeddings in a fashion context. 
	- Establish whether fine-tuning is sufficient to produce better representations, zero-shot, on new data. 
	- Do so in a tractable way.
- Model is trained on over 700k `<image,text>` pairs from the inventory of _Farfetch_.
- report training time, costs, and emissions.