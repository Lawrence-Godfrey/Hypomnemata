https://arxiv.org/abs/2103.00020

### Problem
Training image models on a fixed set of object categories isn't scalable. 
### Solution
- Instead, learn directly from raw text-image pairs scraped from the internet. 
- Shows SOTA performance by training a mode from scratch to predict the correct caption for an image.
- Uses a set of 400M (image, text) pairs collected from the internet.
	- Called **WIT**, for **WebImageText**.
- Zero-shot transfer to downstream tasks.
- "Natural Language Supervision" - learning perception from supervision contained in natural language.
- Learning from natural language has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn’t “just” learn a representation but also connects that representation to language which enables flexible zero-shot transfer.
- Found that training a transformer-based language model to directly predict the caption was an inefficient approach.
	- Instead, train a system to solve the potentially easier proxy task of predicting only which text as a whole is paired with which image and not the exact words of that text.
### Training Objective
![[Pasted image 20250608195443.png]]

 - Given a batch of $N$ (image, text) pairs, CLIP is trained to predict which of the $N × N$ possible (image, text) pairings across a batch actually occurred.
 - Learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximise the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimising the cosine similarity of the embeddings of the $N^2 − N$ incorrect pairings.
 - Text and image embeddings are L2 normalised (euclidean distances lie on the unit hemisphere).
 - Optimise a symmetric cross entropy loss over these similarity scores.
	 - From the perspective of the image, CLIP is trained to identify the correct text out of all possible texts in the batch.
	- Symmetrically, from the perspective of the text, it's trained to identify the correct image out of all images.
 - Known as [InfoNCE] Loss.
 - Linear probes (logistic regression on frozen features) also perform well, indicating CLIP's embeddings are linearly separable and general.