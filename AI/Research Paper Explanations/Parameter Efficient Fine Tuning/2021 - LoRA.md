# LoRA: Low-Rank Adaption of Large Language Models
https://arxiv.org/abs/2106.09685

 - Freeze the pretrained model weights and inject trainable rank decomposition matrices into each layer of the Transformer architecture.
 - Greatly reduces the number of trainable parameters for downstream tasks.

![[Pasted image 20250921100553.png|center]]
                                *Only train A and B*

## Main Idea
 - Learned **over-parametrised** models (models which have far more parameters than strictly necessary to fit their training data) in fact reside on a **low intrinsic dimension**.
	 - I.e., most of the billions of directions in parameter space are irrelevant, only a much smaller number of directions matter for the model’s performance.
 - The change in weights during model adaptation also has a low **intrinsic rank**.
	 - When you fine-tune or adapt a pretrained model to a new task, you change the weights slightly (ΔW). 
	 - LoRA’s hypothesis is that those changes don’t need to span the full high-dimensional space.
	 - **ΔW can be well-approximated by a low-rank matrix** (i.e. it has low “intrinsic rank”).
 - Rather than learning a huge dense update, you only need to learn updates in a much smaller subspace. 
	 - LoRA enforces this by decomposing the update as **ΔW = B A**, where `B` and `A` are thin matrices of rank _r_ (small). 
	 - This reduces the number of trainable parameters massively while still capturing the useful directions of adaptation
## Advantages
 - A pre-trained model can be shared and used to build many small LoRA modules for different tasks.
 - LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters.
 - Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully fine-tuned model, by construction.
 - LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning.
 - 