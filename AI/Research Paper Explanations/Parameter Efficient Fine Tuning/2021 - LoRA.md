# LoRA: Low-Rank Adaption of Large Language Models
https://arxiv.org/abs/2106.09685

 - Freeze the pretrained model weights and inject trainable rank decomposition matrices into each layer of the Transformer architecture.
 - Greatly reduces the number of trainable parameters for downstream tasks.

![[Pasted image 20250921100553.png|center]]
                                *Only train A and B*

## Main Idea
 - Learned **over-parametrised** models (models which have far more parameters than strictly necessary to fit their training data) in fact reside on a **low intrinsic dimension**.
	 - I.e., most of the billions of directions in parameter space are irrelevant, only a much smaller number of directions matter for the model’s performance.
 - The change in weights during model adaptation also has a low **intrinsic rank**.
	 - When you fine-tune or adapt a pretrained model to a new task, you change the weights slightly (ΔW). 
	 - LoRA’s hypothesis is that those changes don’t need to span the full high-dimensional space.
	 - **ΔW can be well-approximated by a low-rank matrix** (i.e. it has low “intrinsic rank”).
 - Rather than learning a huge dense update, you only need to learn updates in a much smaller subspace. 
	 - LoRA enforces this by decomposing the update as **ΔW = B A**, where `B` and `A` are thin matrices of rank _r_ (small). 
	 - This reduces the number of trainable parameters massively while still capturing the useful directions of adaptation
## Advantages
 - A pre-trained model can be shared and used to build many small LoRA modules for different tasks.
 - LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters.
 - Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully fine-tuned model, by construction.
 - LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning.

## Low Rank Decomposition
 - For a pre-trained weight matrix $W_{O} \ele R d x k$ , its update can be represented with a low-rank decomposition $$$W_{O} + \delta W = W_{O}+ BA$$
	 - where $B ∈ R d×r$, $A ∈ R r×k$, and the rank $r$  min(d, k).

## Weight Updates
 - During training, $W0$ is frozen and does not receive gradient updates, while $A$ and $B$ contain trainable parameters. 
 - Note both $W0$ and $∆W = BA$ are multiplied with the same input, and their respective output vectors are summed coordinate-wise. 

## Forward Pass
 - For $h = W0x$, our modified forward pass yields: $h = W0x + ∆W x = W0x + BAx$.

## Transformer Implementation
 - In Transformers, they apply LoRA mainly to attention weights $W_q, W_v$ ​ (query/value projections), freezing MLPs and norms for simplicity. (Ablations later explore more.)

## Initialisation
 - A random Gaussian initialisation is used for $A$ and zero for $B$.
 - $∆W = BA$ is zero at the beginning of training.

## Hyperparameters
$$
h = W_0 x + \frac{\alpha}{r} \cdot (B A x)
$$
- $r$
	- $4$ is the **rank** hyperparameter in LoRA's low-rank decomposition. It defines the "bottleneck" dimension in the trainable matrices $A \in \mathbb{R}^{r \times k}$ and $B \in \mathbb{R}^{d \times r}$, where the full weight update is $\Delta W = BA$.
	- A smaller $r$ (e.g., 1, 4, or 8) keeps the number of trainable parameters low for efficiency, while a larger $r$ (e.g., 32 or 64) makes the adaptation more expressive but increases compute and memory use.
	- From the paper (Section 4.1): $r \ll \min(d, k)$, and it's chosen such that even low values like 1 or 2 suffice for large models (e.g., GPT-3 175B, where full rank could be 12,288). Experiments in the paper show performance plateaus beyond small $r$, and ablations (Section 7.1) confirm $r=4$ often works well.
- Alpha $\alpha$ 
	- $\alpha$ is a **constant scaling hyperparameter** that adjusts the overall magnitude of the LoRA update's contribution relative to the frozen pre-trained weights.
	- It's set once (e.g., to 16 or 32 in the paper's experiments) and treated as fixed with respect to $r$. Higher $\alpha$ emphasizes the LoRA adaptations more strongly, helping balance retention of pre-trained knowledge with learning new task-specific patterns.
	- In practice, a common heuristic is to set $\alpha = r (for a scale of 1) or $r\alpha = 2r$ (for a scale of 2), as this provides a good starting point without over- or under-weighting the updates.

- As $r$ increases, approximates full fine-tuning (unlike adapters, which become MLPs).

## Latency / Pushing Down of Weights
 - The low rank matrices can be added to the actual model weights, essentially pushing down the updates into the model. 
	 - $W = W_0 + B A$
	 - This means no extra computations at inference time. 
 - To switch between different tasks, you can subtract/add low rank matrices. 

## Key Results
- **GLUE: LoRA matches or beats full FT with 0.1-1% of params. E.g., RoBERTa-large: 89.0 avg (LoRA 0.8M params) vs. 88.9 (FT 355M). Outperforms adapters/prefixes.
- **GPT-2**: On E2E NLG (generation quality metrics like BLEU), LoRA with 0.37M params beats full FT (355M) and prefixes/adapters. Similar on WebNLG/DART.
- **GPT-3 175B**: On WikiSQL (acc: 73.8% LoRA vs. 74.8% FT) and SAMSum (ROUGE: 45.9 LoRA vs. 45.1 FT). LoRA uses 0.01% params, trains with 350GB VRAM vs. 1.2TB.

 - LoRA is robust: Higher throughput, no quality drop. Combines well with prefix tuning.

