#flashcards

What does Seq2Seq stand for?
?
Sequence-to-Sequence.

What is the primary use of Seq2Seq models?
?
They are used in natural language processing tasks such as machine translation, text summarization, and chatbot development.

What is the architecture of a Seq2Seq model composed of?
?
An encoder and a decoder.

What does the encoder in a Seq2Seq model do?
?
It processes the input sequence and compresses the information into a context vector.

What is a context vector in a Seq2Seq model?
?
It is a representation of the input sequence that ideally captures its semantic essence, often a single fixed-length vector.

How did the original Seq2Seq model perform with longer contexts?
?
It decreased in performance due to the use of a fixed context vector.

What mechanism was introduced to improve Seq2Seq models for longer contexts?
?
Attention mechanism.

Who were the authors of the original Seq2Seq model paper?
?
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.

In what year was the paper 'Sequence to Sequence Learning with Neural Networks' published?
?
2014.

Why is the quality of the context vector important in Seq2Seq models?
?
Because it significantly impacts the model's performance.

