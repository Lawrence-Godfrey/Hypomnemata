#flashcards

What is a word embedding vector?
?
A word embedding vector is a vector representation of a word that encodes some meaning of the word, allowing for meaningful comparisons between words.

What is required to create a word embedding model?
?
A corpus and an embedding method are required to create a word embedding model.

How should the words in the corpus be organized?
?
The words in the corpus should be organized in the same way as they would be used in the context of interest.

Give an example of an appropriate corpus for generating word embeddings for Shakespeare. 
?
The full original text of Shakespeare should be used as the corpus to generate word embeddings for that context.

Why is it important to use the full original text as a corpus for word embeddings?
?
Using the full original text ensures that the context and usage of the words are accurately represented, which is crucial for generating meaningful and contextually relevant embeddings.

What problems might arise from using study notes to create a word embedding model for Shakespeare?
?
Using study notes may not capture the original context and nuances of the language used in Shakespeare's works, leading to less accurate or meaningful embeddings.

