#flashcards

What does BERT stand for?
?
Bidirectional Encoder Representations from Transformers.

Who introduced BERT and in what year?
?
Google introduced BERT in 2018.

What is the primary training task for BERT?
?
Masked language modelling.

What type of performance is BERT especially known for?
?
Excellent performance on tasks requiring understanding context and relationships between words, such as question answering and sentiment analysis.

What architecture does BERT use?
?
Transformer architecture with a multi-layer encoder.

What does BART stand for?
?
Denoising Autoencoder from Transformer.

Who introduced BART and in what year?
?
Facebook AI introduced BART in 2020.

What is the primary training task for BART?
?
Denoising autoencoding.

What advantage does BART provide in its pre-training task?
?
It encourages the model to learn representations that are robust to noise and variations in the input text.

What is BART especially known for excelling in?
?
Tasks requiring handling complex language, such as text summarisation and machine translation.

What architecture does BART use?
?
Transformer architecture with a multi-layer encoder-decoder.

How does BERT's training method differ from BART's?
?
BERT uses masked language modelling while BART uses denoising autoencoding.

