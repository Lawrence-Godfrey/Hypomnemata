#flashcards

What is Perplexity in the context of language models?
?
Perplexity is a measure of how well a probability distribution or a language model predicts a sample of text.

How is Perplexity calculated?
?
Perplexity is calculated using the formula: PP(W) = P(s_{1}, s_{2}, ..., s_{m})^{-rac{1}{m}}, where W is a test set containing m sentences.

What does a low perplexity score indicate?
?
A low perplexity score, close to 1, indicates a very accurate model that predicts text well.

What perplexity score range is typically considered good for language models?
?
Good models can have perplexity scores between 20 and 50.

Why should perplexity only be used for models with the same vocabulary?
?
Because perplexity is sensitive to the vocabulary size, comparing models with different vocabularies may not yield meaningful insights.

What is log perplexity?
?
Log perplexity is the logarithmic transformation of the perplexity metric that is sometimes used instead.

What is intrinsic evaluation in word embeddings?
?
Intrinsic evaluation assesses word embeddings based on their internal properties and characteristics, such as measuring semantic similarity between words.

Give an example of intrinsic evaluation of word embeddings.
?
An example of intrinsic evaluation is measuring the semantic similarity between words or identifying analogies or word relationships.

What is extrinsic evaluation in the context of word embeddings?
?
Extrinsic evaluation assesses word embeddings based on their performance in specific external tasks or applications.

Can you explain the difference between intrinsic and extrinsic evaluation for word embeddings?
?
Intrinsic evaluation focuses on the quality of word embeddings themselves, while extrinsic evaluation looks at their effectiveness in practical applications.

