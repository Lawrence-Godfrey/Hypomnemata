#flashcards

What does ReLU stand for in neural networks?
?
ReLU stands for Rectified Linear Unit.

How does the ReLU activation function work mathematically?
?
The ReLU activation function works by outputting the maximum of 0 and the input value, expressed as ReLU(x) = max(0, x).

When does a neuron using ReLU activation function become activated?
?
A neuron using the ReLU activation function becomes activated when the sum of the inputs multiplied by their corresponding weights is positive.

What is one advantage of using the ReLU activation function in neural networks?
?
One advantage of ReLU is that it helps mitigate the vanishing gradient problem, allowing models to learn faster and perform better.

Can the ReLU function output negative values?
?
No, the ReLU function outputs only zero or positive values.

In which scenarios might ReLU lead to problems in neural networks?
?
ReLU can cause the 'dying ReLU' problem, where neurons can sometimes get stuck during training and only output zeros.

What is a common alternative to the ReLU activation function?
?
A common alternative to the ReLU activation function is the Leaky ReLU, which allows a small, positive gradient when the input is negative.

