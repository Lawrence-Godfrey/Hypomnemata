The ReLU, or Rectified Linear Unit, is an activation function used in [[Neural Networks]]. The ReLU activation function is a general purpose activation function which only activates the neuron if the sum of the inputs multiplied by their corresponding weights is positive.
$$
ReLU(x) = max(0, x)
$$

![[Pasted image 20231025223215.png]]

