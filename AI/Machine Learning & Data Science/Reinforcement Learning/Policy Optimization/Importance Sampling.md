**Importance Sampling** is a technique from statistics that allows you to compute expectations under one probability distribution using samples from a different distribution. In reinforcement learning, it enables **off-policy learning** - using data collected from an old policy to update a new policy.

## The Core Idea

If you want to compute $\mathbb{E}_{x \sim p}[f(x)]$ but only have samples from distribution $q$, you can reweight them:

$$
\mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)} f(x)\right]
$$

The ratio $\frac{p(x)}{q(x)}$ is called the **importance weight** or **importance ratio**.

## Intuition

The importance weight corrects for the mismatch between distributions:
- If $p(x) > q(x)$: This sample is **underrepresented** in $q$, so weight it more heavily (weight > 1)
- If $p(x) < q(x)$: This sample is **overrepresented** in $q$, so weight it less (weight < 1)

This reweighting makes samples from $q$ behave as if they came from $p$.

## Why RL Needs This

In policy gradient methods like [[REINFORCE]], the gradient is:
$$
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) R_t\right]
$$

This expectation is over trajectories sampled from the **current policy** $\pi_\theta$. 

**The Problem**: After each gradient step, the policy changes to $\pi_{\theta'}$. Trajectories from the old policy $\pi_\theta$ are now technically invalid because they're from the wrong distribution.

**The Solution**: Importance sampling lets you reuse old trajectories by reweighting them.

## Importance Sampling in Policy Gradients

Suppose you collected trajectories from policy $\pi_{\theta_{\text{old}}}$ (the "behavior policy") but want to update $\pi_{\theta_{\text{new}}}$ (the "target policy").

### Trajectory-Level Importance Weight

For a trajectory $\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)$, the probability under a policy is:
$$
P(\tau | \pi) = P(s_0) \prod_{t=0}^{T} \pi(a_t|s_t) P(s_{t+1}|s_t, a_t)
$$

The importance weight is:
$$
\rho(\tau) = \frac{P(\tau | \pi_{\theta_{\text{new}}})}{P(\tau | \pi_{\theta_{\text{old}}})} = \frac{\prod_{t=0}^{T} \pi_{\theta_{\text{new}}}(a_t|s_t) \cancel{P(s_{t+1}|s_t, a_t)}}{\prod_{t=0}^{T} \pi_{\theta_{\text{old}}}(a_t|s_t) \cancel{P(s_{t+1}|s_t, a_t)}}
$$

The environment dynamics cancel out (they're the same for both policies):
$$
\rho(\tau) = \prod_{t=0}^{T} \frac{\pi_{\theta_{\text{new}}}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$

### Off-Policy Gradient Estimate

The gradient becomes:
$$
\nabla_\theta J(\pi_\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T} \underbrace{\prod_{t'=0}^{t} \frac{\pi_\theta(a_{t'}|s_{t'})}{\pi_{\theta_{\text{old}}}(a_{t'}|s_{t'})}}_{\text{importance weight}} \nabla_\theta \log \pi_\theta(a_t|s_t) R_t
$$

Now you can use trajectories from $\pi_{\theta_{\text{old}}}$ to compute gradients for $\pi_\theta$!

## Benefits

1. **Sample efficiency**: Reuse data from old policies instead of throwing it away after each update
2. **Off-policy learning**: Learn from experience generated by different policies (even human demonstrations)
3. **Multiple updates**: Perform several gradient steps on the same batch of trajectories
4. **Experience replay**: Store trajectories and sample from them later (though with caveats)

## The Variance Problem

Importance sampling has a critical weakness: **high variance**.

### Why Variance Explodes

Consider a trajectory of length $T=100$ where each action has importance ratio $r_t = \frac{\pi_{\text{new}}(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$.

Even if each $r_t$ is modest (say, between 0.8 and 1.2), the product $\prod_{t=0}^{100} r_t$ can become extreme:
- Best case: $(0.8)^{100} \approx 2 \times 10^{-10}$ (essentially zero)
- Worst case: $(1.2)^{100} \approx 8 \times 10^{7}$ (huge!)

A single trajectory with a huge weight can dominate the gradient estimate, making training unstable.

### When Variance is High

Variance is worst when:
- **Policies differ significantly**: Large difference between $\pi_{\text{new}}$ and $\pi_{\text{old}}$
- **Long trajectories**: More actions = more multiplicative terms
- **Low probability actions**: If $\pi_{\text{old}}(a|s)$ is small, the weight can explode

## Practical Solutions

Modern RL algorithms address importance sampling variance in several ways:

### 1. Per-Step Importance Weights
Instead of multiplying weights across the entire trajectory, use per-step weights:
$$
\nabla_\theta J(\pi_\theta) \approx \frac{1}{N} \sum_{i,t} \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \nabla_\theta \log \pi_\theta(a_t|s_t) A(s_t, a_t)
$$

This introduces some bias but dramatically reduces variance.

### 2. Clipped Importance Weights (PPO)
[[Proximal Policy Optimization]] clips importance ratios to prevent extreme values:
$$
r_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$
$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t A_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_t\right)\right]
$$

Typically $\epsilon = 0.2$, so weights are constrained to $[0.8, 1.2]$.

### 3. Constrained Policy Updates (TRPO)
[[Trust Region Policy Optimization]] limits how much the policy can change using KL divergence:
$$
\text{KL}(\pi_{\theta_{\text{old}}} \| \pi_\theta) \leq \delta
$$

This keeps policies similar, ensuring importance weights stay close to 1.

### 4. Weighted Importance Sampling
Use a self-normalized estimator that divides by the sum of weights:
$$
\mathbb{E}_p[f(x)] \approx \frac{\sum_i \frac{p(x_i)}{q(x_i)} f(x_i)}{\sum_i \frac{p(x_i)}{q(x_i)}}
$$

This reduces variance at the cost of some bias.

## On-Policy vs Off-Policy

**On-Policy** (no importance sampling):
- [[REINFORCE]], vanilla policy gradients
- [[Advantage Estimation/Actor-Critic Methods|A2C/A3C]]
- Must sample from current policy for each update
- Lower sample efficiency but more stable

**Off-Policy** (with importance sampling):
- [[Proximal Policy Optimization|PPO]], [[Trust Region Policy Optimization|TRPO]]
- Off-policy actor-critic (SAC, DDPG)
- Q-learning based methods
- Higher sample efficiency but need variance control

## When to Use Importance Sampling

**Use it when**:
- You want to reuse old data for multiple gradient steps
- Sample collection is expensive (real-world robotics, slow simulators)
- You want to learn from demonstrations or data from other policies

**Be careful when**:
- Policies can change dramatically (use clipping or constraints)
- Trajectories are very long (consider per-step weights)
- You need stable training (on-policy might be safer)

## Connection to Other Concepts

- **[[Advantage Estimation/README|Advantage estimation]]**: Often combined with importance sampling in modern algorithms
- **[[Policy Gradient Optimisation]]**: Importance sampling enables multiple updates per batch
- **[[REINFORCE]]**: Basic on-policy method; importance sampling makes it off-policy
- **Experience replay**: Requires importance sampling to correct for off-policy data

## Key Takeaway

Importance sampling is the mathematical bridge between on-policy and off-policy learning in RL. It enables sample-efficient algorithms by allowing data reuse, but requires careful variance control through techniques like clipping (PPO) or constraints (TRPO) to remain stable.
