REINFORCE is a fundamental policy gradient algorithm in reinforcement learning. It provides a way to estimate the gradient of the expected return with respect to the policy parameters using sampled trajectories. This method is also; known as the "Monte Carlo policy gradient" method.

It was introduced by Ronald J. Williams in 1992.

**Pros**:
- Simple to implement
- Works with stochastic policies

**Cons**:
- High variance in gradient estimates
- Requires complete trajectories for each update

## Deriving the Policy Gradient
To derive the policy gradient, we start with the definition of the expected return:
$$
\nabla J(\pi_\theta) = \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
$$
Expand expression:
$$
= \nabla_\theta \int_{\tau} P(\tau | \pi_\theta) R(\tau) d\tau
$$
(in other words, sum over all possible trajectories, weighting each reward by its probability)

Bring gradient under integral:
$$
 = \
\int_{\tau} \nabla_\theta P(\tau | \pi_\theta) R(\tau) d\tau
$$
(This is still problematic. We can't use MC estimation to establish this expectation).

Log-derivative trick:
$$
= \int_{\tau} P(\tau | \pi_\theta) \nabla_\theta \log P(\tau | \pi_\theta) R(\tau) d\tau
$$
Return to expectation notation:
$$
= \mathbb{E}_{\tau \sim \pi_\theta} \left[\nabla_\theta \log P(\tau | \pi_\theta) R(\tau)\right]
$$
Expression for grad-log-prob:
$$
= \mathbb{E}_{\tau \sim \pi_\theta} \left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)\right]
$$
This final expression tells us we can estimate the gradient by:
1. **Sample trajectories**: Just run your current policy and collect trajectories (e.g., play 100 games)
2. **Compute a simple formula**: For each trajectory, compute $\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau)$
3. **Average**: Average these values over all sampled trajectories to get an estimate of the policy gradient
## Intuitive Interpretation
The expression $\nabla_\theta log \pi_\theta(a_t | s_t) R(\tau)$ means:

**"Push the policy to increase the probability of taking action $a_t$ in state $s_t$, weighted by how good the trajectory was."**

Breaking it down:
 - $\nabla_\theta \log \pi_\theta(a_t | s_t)$: Direction to change parameters to make action $a_t$ more likely in state $s_t$.
 - $R(\tau)$: How good was the entire trajectory?
 - *Product*: Scale the update by trajectory quality. 
## Log-Derivative Trick
The log-derivative trick (also called the "likelihood ratio trick" or [[REINFORCE]] trick) is a clever algebraic manipulation. Here's how it works:
### The Core Identity
The trick uses the fact that:
$$
\nabla_\theta log f(\theta) = \frac{\nabla_\theta f(\theta)}{f(\theta)}
$$
Or equivalently:
$$
\nabla_\theta f(\theta) = f(\theta) \nabla_\theta log f(\theta)
$$
Apply the log-derivative trick with $f(\theta) = P(\tau | \pi_\theta)$:
$$
\nabla_\theta P(\tau | \pi_\theta) = P(\tau | \pi_\theta) \nabla_\theta \log P(\tau | \pi_\theta)
$$
### Why This is Useful
This transformation is crucial because:
1. Makes it an expectation again: Now you have $P(\tau | \pi_\theta)$ as a probability weight, so you can write it as an expectation over trajectories:
$$
\mathbb{E}_{\tau \sim \pi_\theta} \left[\nabla_\theta \log P(\tau | \pi_\theta) R(\tau)\right]
$$
2. Enables sampling: You can estimate this expectation using samples of trajectories generated by the current policy instead of needing to compute it over all possible trajectories:
$$
\nabla_\theta J(\pi_\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \log P(\tau_i | \pi_\theta) R(\tau_i)
$$