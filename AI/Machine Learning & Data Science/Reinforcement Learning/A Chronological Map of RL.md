
**1933–1952**: [[Bandits|Bandits]]: sequential decision under uncertainty — [[Thompson Sampling|Thompson]] (1933), Robbins (1952), [[Upper Confidence Bound (UCB)|UCB]]/EXP (2000s).
**1950s**: Foundations: dynamic programming & [[Markov Decision Processes]] (MDPs) — [[Bellman Equation|Bellman]].
**1980s**: Early RL / Monte Carlo / Temporal Difference — Sutton, Barto.
**Late 1980s–1990s**: Tabular algorithms: Q-learning, SARSA — Watkins, Rummery.
**1990s**: [[Policy Gradient Optimisation|Policy gradients]] / [[REINFORCE]] / Actor-Critic — Williams; Konda & Tsitsiklis.
**1990s–2015**: Natural gradients / trust regions — Amari; Kakade; Schulman.
**2013–2015**: Deep RL revolution: DQN & value-based deep methods — DeepMind.
**2015–2017**: On-policy actor-critic & stabilisers: A3C, TRPO, PPO — Mnih, Schulman.
**2015–2018**: Continuous-control & off-policy deep methods: DDPG, SAC.
**2017–2022**: Preference learning & [[RLHF]]— Christiano et al. (2017) → InstructGPT / OpenAI (2020–2022).
**Since ~2018**: Practical trends: stability, sample efficiency, offline RL, safe/RL explainability.